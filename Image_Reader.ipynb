{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projeto Final de Ciência dos Dados. ( PkmnID)\n",
    "\n",
    "## Algoritmo de predição da categoria de Pokémons por meio de suas imagens.\n",
    "### O algoritmo realiza a extração e a clusterização de features de imagens por meio do método \\\"Bag of Visual Words\\\" (BOVW),classifica-as utilizando o método de machine learning \\\"Random Forest\\\" e prevê a categoria de Pokémons por meio de novas imagens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "TRAIN_DIR = 'Assets/Final/Train_base'\n",
    "SUPER_TRAIN_DIR = 'Assets/Final/Train_super'\n",
    "TEST_DIR = 'Assets/Final/Test'\n",
    "CLASSES = os.listdir(TRAIN_DIR)\n",
    "\n",
    "NUM_CLUSTERS = 40"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Máquina de controle:\n",
    "\n",
    "# Re-adquiri as features das imagens.\n",
    "READ_IMAGES = 0\n",
    "\n",
    "# Cria arquivo .npy para dicionário de Features.\n",
    "UPDATE_FILES = 0\n",
    "\n",
    "# Constrói dataframe com dados do dicionário de features.\n",
    "CREATE_FEATURE_DATAFRAME = 1\n",
    "\n",
    "# Mostra em quais 'n' pokémons, cada feature é mais proeminente.\n",
    "SHOW_TOP_N_FOR_FEATURES = 1\n",
    "\n",
    "# Re-treina os models.\n",
    "FIT_MODELS = 1\n",
    "\n",
    "# Produz matrizes de confusão para todos os modelos.\n",
    "PLOT_CONFUSION_MATRIXES = 1\n",
    "\n",
    "# Produz uma lista com a métrica precision@n para todos os modelos.\n",
    "PRECISION_AT_N = 1\n",
    "\n",
    "# Roda o modelo combinado dos 3 modelos originais.\n",
    "RUN_SUPER_MODEL = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1- Extração de features de imagens: Bag of Visual Words\n",
    "### Uma vez que o dataset se trata de um conjunto de imagens de diferentes Pokémons, é necessário inicialmente extrair features dessas imagens, através do método \"Bag of Visual Words\".\n",
    "### Com as imagens transformadas em features clusterizadas, elas são separadas em categorias de treino e teste, que serão utilizadas posteriormente pelo algoritmo de machine learning.\n",
    "### O código abaixo realiza essas duas etapas:\n",
    "#### Obs: Código produzido com a assistência do Prof. Fábio Ayres"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_names(TRAIN_DIR = TRAIN_DIR, SUPER_TRAIN_DIR = SUPER_TRAIN_DIR, TEST_DIR = TEST_DIR):\n",
    "\n",
    "    TRAIN_IMG = []\n",
    "    TEST_IMG = []\n",
    "    TRAIN_LABEL = []\n",
    "    TEST_LABEL = []\n",
    "    SUPER_TRAIN_IMG = []\n",
    "    SUPER_TRAIN_LABEL = []\n",
    "\n",
    "    for train, sup_train, test in zip(os.listdir(TRAIN_DIR), os.listdir(SUPER_TRAIN_DIR), os.listdir(TEST_DIR)): \n",
    "        #Tecnicamente são iguais, mas não custa garantir.\n",
    "        dir_train = os.listdir(os.path.join(TRAIN_DIR,train))\n",
    "        dir_sup_train = os.listdir(os.path.join(SUPER_TRAIN_DIR,sup_train))\n",
    "        dir_test = os.listdir(os.path.join(TEST_DIR,test))\n",
    "        for img_train, sup_img_train, img_test in zip(dir_train, dir_sup_train, dir_test):\n",
    "            TRAIN_IMG.append(os.path.join(TRAIN_DIR, train, img_train))\n",
    "            TEST_IMG.append(os.path.join(TEST_DIR, test, img_test))\n",
    "            TRAIN_LABEL.append(train)\n",
    "            TEST_LABEL.append(test)\n",
    "            SUPER_TRAIN_IMG.append(os.path.join(SUPER_TRAIN_DIR, sup_train, sup_img_train))\n",
    "            SUPER_TRAIN_LABEL.append(sup_train)\n",
    "\n",
    "    return TRAIN_IMG, TEST_IMG, SUPER_TRAIN_IMG, SUPER_TRAIN_LABEL, TRAIN_LABEL, TEST_LABEL\n",
    "\n",
    "def cria_vocabulario(imagens, num_clusters):\n",
    "    km = cv2.BOWKMeansTrainer(num_clusters)\n",
    "    akaze = cv2.KAZE_create()\n",
    "    for p in imagens:\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.ones(img.shape)\n",
    "        kp, desc = akaze.detectAndCompute(img, mask)\n",
    "        km.add(desc)\n",
    "    return km.cluster()\n",
    "\n",
    "def representa(vocab, img):\n",
    "    kaze = cv2.KAZE_create()\n",
    "    kp = kaze.detect(img)\n",
    "    bowdesc = cv2.BOWImgDescriptorExtractor(kaze, cv2.FlannBasedMatcher())\n",
    "    bowdesc.setVocabulary(vocab)\n",
    "    return bowdesc.compute(img, kp)\n",
    "\n",
    "def transforma_imagens(imagens, vocab):\n",
    "    X = []\n",
    "    for p in imagens:\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        X.append(representa(vocab, img).flatten())\n",
    "    return np.array(X)\n",
    "\n",
    "def show_example(path = os.listdir(\"Testes/Testes/\")[0], plot = True):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, dsize=(120, 120))\n",
    "    if Plot:\n",
    "        plt.imshow(img_resized, cmap='gray', vmin=0, vmax=255)\n",
    "    return representa(vocab, img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dict = {}\n",
    "Test_Dict = {}\n",
    "Super_Train_Dict = {}\n",
    "TRAIN_IMG, TEST_IMG, SUPER_TRAIN_IMG, SUPER_TRAIN_LABEL, TRAIN_LABEL, TEST_LABEL = get_img_names()\n",
    "\n",
    "if READ_IMAGES:       \n",
    "    vocab = cria_vocabulario(TRAIN_IMG, NUM_CLUSTERS)\n",
    "    for pkmn in os.listdir(TRAIN_DIR):\n",
    "        Train_Dict[pkmn] = transforma_imagens([os.path.join(TRAIN_DIR, pkmn, n) for n in os.listdir(os.path.join(TRAIN_DIR,pkmn))], vocab)   \n",
    "    for pkmn in os.listdir(TEST_DIR):\n",
    "        Test_Dict[pkmn] = transforma_imagens([os.path.join(TEST_DIR, pkmn, n) for n in os.listdir(os.path.join(TEST_DIR,pkmn))], vocab)\n",
    "    for pkmn in os.listdir(SUPER_TRAIN_DIR):\n",
    "        Super_Train_Dict[pkmn] = transforma_imagens([os.path.join(SUPER_TRAIN_DIR, pkmn, n) for n in os.listdir(os.path.join(SUPER_TRAIN_DIR,pkmn))], vocab)  \n",
    "    \n",
    "    if UPDATE_FILES:\n",
    "        if 'files' not in os.listdir(\"Assets/\"):\n",
    "            os.mkdir('Assets/files')\n",
    "        np.save('Assets/files/Features_Train', Train_Dict)\n",
    "        np.save('Assets/files/Features_Test', Test_Dict)\n",
    "        np.save('Assets/files/Features_Super_Train', Super_Train_Dict)\n",
    "        np.save('Assets/files/Bag_of_Visual_Words', vocab)\n",
    "            \n",
    "\n",
    "else:\n",
    "        Train_Dict = np.load('Assets/files/Features_Train.npy', allow_pickle=True)[()]\n",
    "        Super_Train_Dict = np.load('Assets/files/Features_Super_Train.npy', allow_pickle=True)[()]\n",
    "        Test_Dict = np.load('Assets/files/Features_Test.npy', allow_pickle=True)[()]\n",
    "        vocab = np.load('Assets/files/Bag_of_Visual_Words.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors = np.array([v for k,v in Train_Dict.items()])\n",
    "X_super_train_vectors = np.array([v for k,v in Super_Train_Dict.items()])\n",
    "X_test_vectors = np.array([v for k,v in Test_Dict.items()]) \n",
    "y_train_vectors = [] \n",
    "y_super_train_vectors = []\n",
    "y_test_vectors = []\n",
    "\n",
    "for k in Train_Dict:\n",
    "    x = []\n",
    "    for v in Train_Dict[k]:\n",
    "        x.append(k)\n",
    "    x = np.array(x)\n",
    "    y_train_vectors.append(x)\n",
    "\n",
    "for k in Super_Train_Dict:\n",
    "    x3 = []\n",
    "    for v in Super_Train_Dict[k]:\n",
    "        x3.append(k)\n",
    "    x3 = np.array(x3)\n",
    "    y_super_train_vectors.append(x3)\n",
    "\n",
    "for k in Test_Dict:\n",
    "    x2 = []\n",
    "    for v in Test_Dict[k]:\n",
    "        x2.append(k)\n",
    "    x2 = np.array(x2)\n",
    "    y_test_vectors.append(x2)\n",
    "\n",
    "y_train_vectors = np.array(y_train_vectors)\n",
    "y_super_train_vectors = np.array(y_super_train_vectors)\n",
    "y_test_vectors = np.array(y_test_vectors)\n",
    "\n",
    "X_train, X_super_train, X_test, y_train, y_super_train, y_test = [], [], [], [], [], []\n",
    "\n",
    "for pkmn, matrix in zip(CLASSES, X_train_vectors):\n",
    "    for feature_vector in matrix:\n",
    "        X_train.append(feature_vector)\n",
    "        y_train.append(pkmn)\n",
    "\n",
    "for pkmn_super, matrix_super in zip(CLASSES, X_super_train_vectors):\n",
    "    for feature_vector_super in matrix_super:\n",
    "        X_super_train.append(feature_vector_super)\n",
    "        y_super_train.append(pkmn_super)\n",
    "\n",
    "for pkmn_test, matrix_test in zip(CLASSES, X_test_vectors):\n",
    "    for feature_vector_test in matrix_test:\n",
    "        X_test.append(feature_vector_test)\n",
    "        y_test.append(pkmn_test)\n",
    "\n",
    "X_train, X_super_train, X_test = np.array(X_train), np.array(X_super_train), np.array(X_test) \n",
    "y_train, y_super_train, y_test = np.array(y_train), np.array(y_super_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 - Análise Exploratória:\n",
    "### Para realizar a análise exploratória seguiremos alguns passos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2 - Criar um dataframe para trabalhar melhor com o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_FEATURE_DATAFRAME:    \n",
    "    lista = []\n",
    "    for k in Train_Dict:\n",
    "        mean = []\n",
    "        for i in range(NUM_CLUSTERS):\n",
    "            mean.append(pd.Series(Train_Dict[k][:,i]).mean())\n",
    "        lista.append(mean)\n",
    "    df_medias = pd.DataFrame(lista, index = CLASSES)\n",
    "else:\n",
    "    df_medias = pd.DataFrame([[1],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tabela das frequências relativas médias de cada feature por pokémon:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.3 - Calculando os valores médios dos dados:\n",
    "### Nesta etapa foi calculado os valores médios dos dados, e em sequência foram aproximados do ponto (0,0), origem do sistema."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias = df_medias - (1/NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "normas = (df_medias*df_medias).sum(axis=1)\n",
    "for m in normas.index:\n",
    "    df_medias.loc[m] = df_medias.loc[m]/np.sqrt(normas[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.4 - Comparação entre os Pokémons:\n",
    "### Com base nos valores calculados anteriormente, foi criada a tabela seguinte, que mostra o quanto os Pokémons são semelhantes entre si, sendo 1 a semelhança máxima, e -1 o oposto."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compara = df_medias.dot(df_medias.transpose())\n",
    "df_compara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Podemos observar que alguns Pokémons possuem muitas semelhanças pois apresentam as mesmas features em abundância (na média).\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_TOP_N_FOR_FEATURES:    \n",
    "    monstros = []\n",
    "    for feat in range(NUM_CLUSTERS):\n",
    "        monstros.append(sorted(df_medias.nlargest(n=5, columns=[feat]).index) + [feat])\n",
    "    x = sorted(monstros)\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A soma das colunas da tabela anterior mostra quais Pokémons são mais difíceis de distinguir."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compara.sum(axis = 1).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 - \"Machine Learning\" e Classificação:\n",
    "### O método de aprendizado de máquina e classificação utilizado foi o \"Random Forest Classifier\", assim como \"Logistic Regression Classifier\" e \"KNearesNeighbors Classifier\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_MODELS:\n",
    "    # Random forest\n",
    "    randf = RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators = 100)\n",
    "    randf.fit(X_train, y_train)\n",
    "\n",
    "    # KNearestNeighbors\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    \n",
    "    # Decision Tree\n",
    "    tree = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Logistic regression, não utilizado.\n",
    "    #logit = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, y_train)\n",
    "    #logit.fit(X_train, y_train);\n",
    "\n",
    "    # Método Nearest Centroid, não utilizado.\n",
    "    # from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "    # clf4 = NearestCentroid()\n",
    "    # clf4.fit(X_train, y_train)\n",
    "\n",
    "    # Método Support Vector Machine, não utilizado.\n",
    "    # from sklearn import svm\n",
    "    # clf5 = svm.SVC(gamma='scale')\n",
    "    # clf5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abaixo armazenamos os modelos numa estrutura que nos será mais acessível."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FIT_MODELS:\n",
    "    models = {'Random Forest': randf,\n",
    "            'KNearestNeighbors': neigh,\n",
    "            'Decision Tree': tree}\n",
    "else:\n",
    "    models = {0:0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 - Análise das classificações realizadas pelo modelo:\n",
    "### A matriz de confusão abaixo mostra em mais detalhes os erros e acertos do classificador. É possível identificar que na maioria das vezes que o modelo falhou, ele identificou erroneamente o Pokémon como sendo uma \"Jigglypuff\" ou um \"Arcanine\".\n",
    "\n",
    "#### Obs: A função *plot_confusion_matrix* abaixo não é de nossa autoria, e sua versão original pode ser encontrada no seguinte endereço: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=True,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Greens):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (16,16))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def multi_confusion_mtx(X_test, y_test, model_dict):\n",
    "    for k,v in model_dict.items():\n",
    "        plot_confusion_matrix(y_test, v.predict(X_test), classes=v.classes_,\n",
    "                            title='Normalized confusion matrix for model %s' % k)\n",
    "        plt.show()\n",
    "\n",
    "def precision_at_n(model, n = 3, X_test = X_test, y_test = y_test):\n",
    "    prediction = model.predict_proba(X_test)\n",
    "    prediction_at_n = []\n",
    "    for k in prediction:\n",
    "        prediction_at_n.append(pd.Series(k, index = CLASSES).nlargest(n).index.tolist())\n",
    "    hits, misses = 0, 0\n",
    "    for guess, answer in zip(prediction_at_n, y_test):\n",
    "        if answer in guess:\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    return hits/(hits+misses)\n",
    "\n",
    "def show_guess(path, model):\n",
    "    return pd.Series(model.predict_proba(show_example(path))[0], index = model.classes_).sort_values(ascending = False)\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_CONFUSION_MATRIXES and FIT_MODELS:\n",
    "    multi_confusion_mtx(X_test, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRECISION_AT_N and FIT_MODELS:\n",
    "    pprint({model_name: precision_at_n(model) for (model_name, model) in models.items()})"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(\"%s score of: %.5f\" % (model_name, model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nada mal! Mas podemos fazer melhor?\n",
    "\n",
    "## Stacking: Combinando os modelos em um(a) supermodelo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lower_models(X, models = models):    \n",
    "    super_matrix = {}\n",
    "    for model_name, model in models.items():\n",
    "        super_matrix[model_name] = model.predict_proba(X).tolist()\n",
    "    super_X_train_comb = []\n",
    "    for vector_1, vector_2, vector_3 in zip(super_matrix['Random Forest'], super_matrix['KNearestNeighbors'], super_matrix['Decision Tree']):    \n",
    "        super_X_train_comb.append(np.concatenate((vector_1, vector_2, vector_3), axis=None))\n",
    "    return np.array(super_X_train_comb)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_X_train_comb = predict_lower_models(X_super_train)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_clf = RandomForestClassifier(n_jobs=-1, random_state= 0, n_estimators = 200)\n",
    "super_clf.fit(super_X_train_comb, y_super_train)\n",
    "super_clf.score(predict_lower_models(X_test), y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precison at N do Super Modelo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_n(super_clf, X_test = predict_lower_models(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matriz de confusão do Super Modelo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, super_clf.predict(predict_lower_models(X_test)), classes = CLASSES, title='Normalized confusion matrix for Super Model', cmap = plt.cm.Greens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}