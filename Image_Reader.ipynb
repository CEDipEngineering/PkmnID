{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final de Ciência dos Dados. ( PkmnID)\n",
    "\n",
    "## Algoritmo de predição da categoria de Pokémons por meio de suas imagens.\n",
    "### O algoritmo realiza a extração e a clusterização de features de imagens por meio do método \\\"Bag of Visual Words\\\" (BOVW),classifica-as utilizando o método de machine learning \\\"Random Forest\\\" e prevê a categoria de Pokémons por meio de novas imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Extração de features de imagens: Bag of Visual Words\n",
    "### Uma vez que o dataset se trata de um conjunto de imagens de diferentes Pokémons, é necessário inicialmente extrair features dessas imagens, através do método \"Bag of Visual Words\".\n",
    "### Com as imagens transformadas em features clusterizadas, elas são separadas em categorias de treino e teste, que serão utilizadas posteriormente pelo algoritmo de machine learning.\n",
    "### O código abaixo realiza essas duas etapas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'Assets//Data_Train'\n",
    "TEST_DIR = 'Assets//Data_Test'\n",
    "\n",
    "NUM_CLUSTERS = 50\n",
    "\n",
    "TRAIN_IMG = []\n",
    "TEST_IMG = []\n",
    "TRAIN_LABEL = []\n",
    "TEST_LABEL = []\n",
    "\n",
    "for train, test in zip(os.listdir(TRAIN_DIR), os.listdir(TEST_DIR)): #Tecnicamente são iguais, mas não custa garantir.\n",
    "    for img_train, img_test in zip(os.listdir(os.path.join(TRAIN_DIR,train)), os.listdir(os.path.join(TEST_DIR,test))):\n",
    "        TRAIN_IMG.append(os.path.join(TRAIN_DIR,train,img_train))\n",
    "        TEST_IMG.append(os.path.join(TEST_DIR,test,img_test))\n",
    "        TRAIN_LABEL.append(train)\n",
    "        TEST_LABEL.append(test)\n",
    "\n",
    "\n",
    "def cria_vocabulario(imagens, num_clusters):\n",
    "    km = cv2.BOWKMeansTrainer(num_clusters)\n",
    "    akaze = cv2.KAZE_create()\n",
    "    for p in imagens:\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.ones(img.shape)\n",
    "        kp, desc = akaze.detectAndCompute(img, mask)\n",
    "        km.add(desc)\n",
    "    return km.cluster()\n",
    "\n",
    "def representa(vocab, img):\n",
    "    kaze = cv2.KAZE_create()\n",
    "    kp = kaze.detect(img)\n",
    "    bowdesc = cv2.BOWImgDescriptorExtractor(kaze, cv2.FlannBasedMatcher())\n",
    "    bowdesc.setVocabulary(vocab)\n",
    "    return bowdesc.compute(img, kp)\n",
    "\n",
    "def transforma_imagens(imagens, vocab):\n",
    "    X = []\n",
    "    for p in imagens:\n",
    "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "        X.append(representa(vocab, img).flatten())\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "\n",
    "vocab = cria_vocabulario(TRAIN_IMG, NUM_CLUSTERS)\n",
    "X_train = transforma_imagens(TRAIN_IMG, vocab)\n",
    "X_test = transforma_imagens(TEST_IMG, vocab)\n",
    "y_train = TRAIN_LABEL\n",
    "y_test = TEST_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Análise Exploratória:\n",
    "### Para realizar a análise exploratória seguiremos alguns passos:\n",
    "### 2.1 - Extrair histograma:\n",
    "### O código abaixo cria o histograma de frequências relativas das features de todas as imagens do dataset escolhido.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_dir = 'Assets//Data_Filtered_Resized'\n",
    "Hist_Dict = {}\n",
    "for pkmn in os.listdir(origin_dir):\n",
    "    Hist_Dict[pkmn] = []\n",
    "    current_dir = os.path.join(origin_dir,pkmn)\n",
    "    for k, img in enumerate(os.listdir(current_dir)):\n",
    "        Hist_Dict[pkmn].append(show_example(os.path.join(current_dir,img), Plot = False))\n",
    "# print(Hist_Dict['Alakazam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Criar um dataframe para trabalhar melhor com o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = []\n",
    "lista_nomes = os.listdir('Assets/Data_Filtered_Resized')\n",
    "for k in Hist_Dict:\n",
    "    x = pd.Series(Hist_Dict[k]).mean()\n",
    "    x = pd.Series(x[0])\n",
    "    lista.append(x)\n",
    "df_medias = pd.DataFrame(lista, index = lista_nomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela das frequências relativas médias de cada feature por pokémon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Calculando os valores médios dos dados:\n",
    "### Nesta etapa foi calculado os valores médios dos dados, e em sequência foram aproximados do ponto (0,0), origem do sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias = df_medias - (1/40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medias.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normas = (df_medias*df_medias).sum(axis=1)\n",
    "for m in normas.index:\n",
    "    df_medias.loc[m] = df_medias.loc[m]/np.sqrt(normas[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Comparação entre os Pokémons:\n",
    "### Com base nos valores calculados anteriormente, foi criada a tabela seguinte, que mostra o quanto os Pokémons são semelhantes entre si, sendo 1 a semelhança máxima, e -1 o oposto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compara = df_medias.dot(df_medias.transpose())\n",
    "df_compara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos observar que alguns Pokémons possuem muitas semelhanças pois apresentam as mesmas features em abundância (na média).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monstros = []\n",
    "for feat in range(NUM_CLUSTERS):\n",
    "    monstros.append(sorted(df_medias.nlargest(n=5, columns=[feat]).index) + [feat])\n",
    "x = sorted(monstros)\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A soma das colunas da tabela anterior mostra quais Pokémons são mais difíceis de distinguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compara.sum(axis = 1).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators = 100)\n",
    "\n",
    "# Train the Classifier to take the training features and learn how they relate\n",
    "# to the training y (the species)\n",
    "clf.fit(X_train, y_train)\n",
    "scr = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr, 1/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Código retirado de https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=True,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (16,16))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=clf.classes_,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, miss = 0, 0\n",
    "for img, label in zip(TEST_IMG, TEST_LABEL):\n",
    "    rep = representa(vocab, cv2.imread(img))\n",
    "    top3 = pd.Series(clf.predict_proba(rep)[0], index = os.listdir('Assets/Data_Test')).nlargest(3)\n",
    "    if label in top3.index.tolist():\n",
    "        hits += 1\n",
    "    else:\n",
    "        miss += 1\n",
    "        \n",
    "hits, miss, hits/(hits+miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(path = \"Testes/Testes/9.png\", Plot = True):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, dsize=(120, 120))\n",
    "    if Plot:\n",
    "        plt.imshow(img_resized, cmap='gray', vmin=0, vmax=255)\n",
    "    return representa(vocab, img_resized)\n",
    "\n",
    "clf.predict_proba(show_example()), clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia:\n",
    "- Modelo Bag of Visual Words, e parte da análise exploratória produzidos por/com assistência de Fábio Ayres.\n",
    "- Dataset: [Pokémon Gen One](https://www.kaggle.com/thedagger/pokemon-generation-one/data) da plataforma Kaggle.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
