{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Projeto Final de Ciência dos Dados. ( PkmnID)\n","\n","## Algoritmo de predição da categoria de Pokémons por meio de suas imagens.\n","### O algoritmo realiza a extração e a clusterização de features de imagens por meio do método \\\"Bag of Visual Words\\\" (BOVW),classifica-as utilizando o método de machine learning \\\"Random Forest\\\" e prevê a categoria de Pokémons por meio de novas imagens."]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":"!pip install opencv-contrib-python\nimport cv2\nimport os\nimport os.path\nimport numpy as np\nimport math\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nnp.random.seed(0)"},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":"## Máquina de controle:\n\n# Extrai as features de todas as imagens novamente.\nRE_EXTRACT_FEATURES = 0\n\n# Re-extrai os nomes das imagens (com os caminhos adequados), de cada pasta.\nRE_ACQUIRE_IMG_NAMES = 0\n\n# Re-constrói o dicionário de features que permite a análise exploratória.\nRE_CREATE_FEATURE_DICT = 0\n\n# Constrói dataframe com dados do dicionário de features.\nCREATE_FEATURE_DATAFRAME = 0\n\n# Mostra em quais 'n' pokémons, cada feature é mais proeminente.\nSHOW_TOP_N_FOR_FEATURES = 0\n\n# Re-treina os models.\nFIT_MODELS = 1\n\n# Produz matrizes de confusão para todos os modelos.\nPLOT_CONFUSION_MATRIXES = 0\n\n# Produz uma lista com a métrica precision@n para todos os modelos.\nPRECISION_AT_N = 1\n"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 1- Extração de features de imagens: Bag of Visual Words\n","### Uma vez que o dataset se trata de um conjunto de imagens de diferentes Pokémons, é necessário inicialmente extrair features dessas imagens, através do método \"Bag of Visual Words\".\n","### Com as imagens transformadas em features clusterizadas, elas são separadas em categorias de treino e teste, que serão utilizadas posteriormente pelo algoritmo de machine learning.\n","### O código abaixo realiza essas duas etapas:\n","#### Obs: Código produzido com a assistência do Prof. Fábio Ayres"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":"def get_img_names(TRAIN_DIR = TRAIN_DIR, TEST_DIR = TEST_DIR):\n\n    TRAIN_IMG = []\n    TEST_IMG = []\n    TRAIN_LABEL = []\n    TEST_LABEL = []\n\n    for train, test in zip(os.listdir(TRAIN_DIR), os.listdir(TEST_DIR)): #Tecnicamente são iguais, mas não custa garantir.\n        for img_train, img_test in zip(os.listdir(os.path.join(TRAIN_DIR,train)), os.listdir(os.path.join(TEST_DIR,test))):\n            TRAIN_IMG.append(os.path.join(TRAIN_DIR,train,img_train))\n            TEST_IMG.append(os.path.join(TEST_DIR,test,img_test))\n            TRAIN_LABEL.append(train)\n            TEST_LABEL.append(test)\n\n    return TRAIN_IMG, TEST_IMG, TRAIN_LABEL, TEST_LABEL\n\ndef cria_vocabulario(imagens, num_clusters):\n    km = cv2.BOWKMeansTrainer(num_clusters)\n    akaze = cv2.KAZE_create()\n    for p in imagens:\n        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n        mask = np.ones(img.shape)\n        kp, desc = akaze.detectAndCompute(img, mask)\n        km.add(desc)\n    return km.cluster()\n\ndef representa(vocab, img):\n    kaze = cv2.KAZE_create()\n    kp = kaze.detect(img)\n    bowdesc = cv2.BOWImgDescriptorExtractor(kaze, cv2.FlannBasedMatcher())\n    bowdesc.setVocabulary(vocab)\n    return bowdesc.compute(img, kp)\n\ndef transforma_imagens(imagens, vocab):\n    X = []\n    for p in imagens:\n        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n        X.append(representa(vocab, img).flatten())\n    return np.array(X)\n\ndef show_example(path = os.listdir(\"Testes/Testes/\")[0], Plot = True):\n    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    img_resized = cv2.resize(img, dsize=(120, 120))\n    if Plot:\n        plt.imshow(img_resized, cmap='gray', vmin=0, vmax=255)\n    return representa(vocab, img_resized)\n\nTRAIN_DIR = 'Assets//Data_Train'\nTEST_DIR = 'Assets//Data_Test'\n\nNUM_CLUSTERS = 40\n\nif RE_ACQUIRE_IMG_NAMES:\n    TRAIN_IMG, TEST_IMG, TRAIN_LABEL, TEST_LABEL = get_img_names()\n\nif RE_EXTRACT_FEATURES:\n    vocab = cria_vocabulario(TRAIN_IMG, NUM_CLUSTERS)\n    X_train = transforma_imagens(TRAIN_IMG, vocab)\n    X_test = transforma_imagens(TEST_IMG, vocab)\n    y_train = TRAIN_LABEL\n    y_test = TEST_LABEL"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"## 2 - Análise Exploratória:\n### Para realizar a análise exploratória seguiremos alguns passos:\n### 2.1 - Extrair histograma:\n### O código abaixo cria o histograma de frequências relativas das features de todas as imagens do dataset escolhido."},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":"if RE_CREATE_FEATURE_DICT:\n        origin_dir = 'Assets/Data_Train'\n        Hist_Dict = {}\n        for pkmn in os.listdir(origin_dir):\n                Hist_Dict[pkmn] = []\n                current_dir = os.path.join(origin_dir,pkmn)\n                for k, img in enumerate(os.listdir(current_dir)):\n                        Hist_Dict[pkmn].append(show_example(os.path.join(current_dir,img), Plot = False))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 2.2 - Criar um dataframe para trabalhar melhor com o dataset:"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[],"source":"if CREATE_FEATURE_DATAFRAME:    \n    lista = []\n    lista_nomes = os.listdir('Assets/Data_Train')\n    for k in Hist_Dict:\n        x = pd.Series(Hist_Dict[k]).mean()\n        x = pd.Series(x[0])\n        lista.append(x)\n    df_medias = pd.DataFrame(lista, index = lista_nomes)\nelse:\n    df_medias = pd.DataFrame([[1],[1]])"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## Tabela das frequências relativas médias de cada feature por pokémon:"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":"df_medias.head()"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 2.3 - Calculando os valores médios dos dados:\n","### Nesta etapa foi calculado os valores médios dos dados, e em sequência foram aproximados do ponto (0,0), origem do sistema."]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":"df_medias = df_medias - (1/40)"},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":"df_medias.sum(axis=1)"},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[],"source":"normas = (df_medias*df_medias).sum(axis=1)\nfor m in normas.index:\n    df_medias.loc[m] = df_medias.loc[m]/np.sqrt(normas[m])"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 2.4 - Comparação entre os Pokémons:\n","### Com base nos valores calculados anteriormente, foi criada a tabela seguinte, que mostra o quanto os Pokémons são semelhantes entre si, sendo 1 a semelhança máxima, e -1 o oposto."]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[],"source":"df_compara = df_medias.dot(df_medias.transpose())\ndf_compara"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Podemos observar que alguns Pokémons possuem muitas semelhanças pois apresentam as mesmas features em abundância (na média).\""]},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":"if SHOW_TOP_N_FOR_FEATURES:    \n    monstros = []\n    for feat in range(NUM_CLUSTERS):\n        monstros.append(sorted(df_medias.nlargest(n=5, columns=[feat]).index) + [feat])\n    x = sorted(monstros)\n    pprint(x)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### A soma das colunas da tabela anterior mostra quais Pokémons são mais difíceis de distinguir."]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[],"source":"df_compara.sum(axis = 1).sort_values(ascending = False)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"## 3 - \"Machine Learning\" e Classificação:\n### O método de aprendizado de máquina e classificação utilizado foi o \"Random Forest Classifier\", assim como \"Logistic Regression Classifier\" e \"KNearesNeighbors Classifier\"."},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[],"source":"if FIT_MODELS:\n    # Random forest\n    randf = RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators = 100)\n    randf.fit(X_train, y_train)\n\n    # KNearestNeighbors\n    neigh = KNeighborsClassifier(n_neighbors=5)\n    neigh.fit(X_train, y_train)\n\n    # Logistic regression\n    logit = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(X_train, y_train)\n    logit.fit(X_train, y_train);\n\n    # Método Nearest Centroid, não utilizado.\n    # from sklearn.neighbors.nearest_centroid import NearestCentroid\n    # clf4 = NearestCentroid()\n    # clf4.fit(X_train, y_train)\n\n    # Método Support Vector Machine, não utilizado.\n    # from sklearn import svm\n    # clf5 = svm.SVC(gamma='scale')\n    # clf5.fit(X_train, y_train)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"### Abaixo armazenamos os modelos numa estrutura que nos será mais acessível."},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":"if FIT_MODELS:\n    models = {'Random Forest': randf,\n            'KNearestNeighbors': neigh,\n            'Losgistic Regression': logit}\nelse:\n    models = {0:0}"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"## 3.1 - Análise das classificações realizadas pelo modelo:\n### A matriz de confusão abaixo mostra em mais detalhes os erros e acertos do classificador. É possível identificar que na maioria das vezes que o modelo falhou, ele identificou erroneamente o Pokémon como sendo uma \"Jigglypuff\" ou um \"Arcanine\".\n\n#### Obs: A função *plot_confusion_matrix* abaixo não é de nossa autoria, e sua versão original pode ser encontrada no seguinte endereço: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[],"source":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=True,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    fig, ax = plt.subplots(figsize = (16,16))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\ndef multi_confusion_mtx(X_test, y_test, model_dict):\n    for k,v in model_dict.items():\n        plot_confusion_matrix(y_test, v.predict(X_test), classes=v.classes_,\n                            title='Normalized confusion matrix for model %s' % k)\n        plt.show()\n\ndef precision_at_n(model):\n    hits, miss = 0, 0\n    for img, label in zip(TEST_IMG, TEST_LABEL):\n        rep = representa(vocab, cv2.imread(img))\n        top3 = pd.Series(model.predict_proba(rep)[0], index = os.listdir('Assets/Data_Test')).nlargest(3)\n        if label in top3.index.tolist():\n            hits += 1\n        else:\n            miss += 1\n            \n    return hits, miss, hits/(hits+miss)\n\ndef show_guess(path, model):\n    return pd.Series(model.predict_proba(show_example(path))[0], index = model.classes_).sort_values(ascending = False)\n\nnp.set_printoptions(precision=2)"},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[],"source":"if PLOT_CONFUSION_MATRIXES and FIT_MODELS:\n    multi_confusion_mtx(X_test, y_test, models)"},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":"if PRECISION_AT_N and FIT_MODELS:\n    pprint({model_name: precision_at_n(model) for (model_name, model) in models.items()})"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}